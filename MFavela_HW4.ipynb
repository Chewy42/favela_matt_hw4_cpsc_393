{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 393 Machine Learning - Homework 4\n",
    "By Matthew Favela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'TypeAliasType' could not be imported from 'C:\\Users\\mfave\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\typing_extensions.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import keras as kb\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "from random import randint\n",
    "from pickle import load\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"GPU acceleration enabled\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")\n",
    "\n",
    "# Enable parallel CPU execution\n",
    "tf.config.threading.set_intra_op_parallelism_threads(16)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(16)\n",
    "\n",
    "# Use mixed precision for faster computation\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "token_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        return text\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback to latin-1 if UTF-8 fails\n",
    "        with open(filename, 'r', encoding='latin-1') as file:\n",
    "            text = file.read()\n",
    "        return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    doc = doc.replace('--', ' ')\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(data)\n",
    "\n",
    "in_filename = \"alices_adventure_in_wonderland.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:token_length])\n",
    "\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:token_length])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "length = token_length + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "out_filename = '{}_sequence.txt'.format(in_filename)\n",
    "save_doc(sequences, out_filename)\n",
    "\n",
    "in_filename = '{}_sequence.txt'.format(in_filename)\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = kb.utils.to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, token_length, input_shape=(seq_length,)))\n",
    "model.add(tf.keras.layers.LSTM(100))\n",
    "model.add(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "model.save('model.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = kb.preprocessing.sequence.pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        yhat = np.argmax(model.predict(encoded, verbose = 0), axis=-1)\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(\"SEED TEXT:\", seed_text)\n",
    "\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(\"GENDERATED TEXT:\", generated + '\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
